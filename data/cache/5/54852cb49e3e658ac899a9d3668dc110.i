a:373:{i:0;a:3:{i:0;s:14:"document_start";i:1;a:0:{}i:2;i:0;}i:1;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:64:"Spatial modelling of bumblebees & butterflies in Southern Norway";i:1;i:1;i:2;i:1;}i:2;i:1;}i:2;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:1;}i:2;i:1;}i:3;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1;}i:4;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:22:"markdowku_boldasterisk";i:1;a:2:{i:0;i:1;i:1;s:2:"**";}i:2;i:1;i:3;s:2:"**";}i:2;i:80;}i:5;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:22:"markdowku_boldasterisk";i:1;a:2:{i:0;i:3;i:1;s:49:"Jens Åström, Department of Ecology, SLU, SWEDEN";}i:2;i:3;i:3;s:49:"Jens Åström, Department of Ecology, SLU, SWEDEN";}i:2;i:82;}i:6;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:22:"markdowku_boldasterisk";i:1;a:2:{i:0;i:4;i:1;s:2:"**";}i:2;i:4;i:3;s:2:"**";}i:2;i:131;}i:7;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:133;}i:8;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:134;}i:9;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:136;}i:10;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:136;}i:11;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:136;}i:12;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:136;}i:13;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:31:" Download the project proposal ";}i:2;i:140;}i:14;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:82:"http://www.spatial-ecology.net/ost4sem/project/unidk2010/MODB&B/documents/Jens.pdf";i:1;s:10:" Jens.pdf ";}i:2;i:171;}i:15;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:".";}i:2;i:269;}i:16;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:270;}i:17;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:270;}i:18;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:270;}i:19;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:270;}i:20;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:271;}i:21;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:273;}i:22;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:274;}i:23;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:274;}i:24;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:12:"INTRODUCTION";i:1;i:2;i:2;i:274;}i:2;i:274;}i:25;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:274;}i:26;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:274;}i:27;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:22:"markdowku_boldasterisk";i:1;a:2:{i:0;i:1;i:1;s:2:"**";}i:2;i:1;i:3;s:2:"**";}i:2;i:302;}i:28;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:22:"markdowku_boldasterisk";i:1;a:2:{i:0;i:3;i:1;s:32:"I work together with Thijs, see ";}i:2;i:3;i:3;s:32:"I work together with Thijs, see ";}i:2;i:304;}i:29;a:3:{i:0;s:12:"internallink";i:1;a:2:{i:0;s:10:"dk10marine";i:1;s:10:" MARINE   ";}i:2;i:336;}i:30;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:22:"markdowku_boldasterisk";i:1;a:2:{i:0;i:3;i:1;s:79:". We use basically the same workflow and methods, but with different data sets.";}i:2;i:3;i:3;s:79:". We use basically the same workflow and methods, but with different data sets.";}i:2;i:362;}i:31;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:22:"markdowku_boldasterisk";i:1;a:2:{i:0;i:4;i:1;s:2:"**";}i:2;i:4;i:3;s:2:"**";}i:2;i:441;}i:32;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:443;}i:33;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:445;}i:34;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:34:"General framework of this analysis";i:1;i:3;i:2;i:445;}i:2;i:445;}i:35;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:445;}i:36;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:445;}i:37;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:354:"Model the abundance and species richness of bumblebees and butterflies as explained by environmental and spatial characteristics. Explanatory variables exists both in GIS layers and Excel sheets filled in by those who conducted the study. The data was kindly made available to me by the project managers at Norwegian Institute for Nature Research, NINA. ";}i:2;i:490;}i:38;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:844;}i:39;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:846;}i:40;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:18:"Project objectives";i:1;i:3;i:2;i:846;}i:2;i:846;}i:41;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:846;}i:42;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:846;}i:43;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:891:"It is commonplace in the ecology to try and explain abundances and diversity patterns from a limited number of predictive variables observed in the field. Often, this is done just to understand the importance of these predictive variables, and not primarily to predict flora and fauna communities in new areas and points in time. Some times the predictive variables are the direct result of manipulations by the researcher. The analyses often stop after having tested if the parameters are different from zero, but hopefully they also contain estimations of effect size. How well your models represent the world can be checked by looking at how much variation in the data they explain. However, sometimes these measures are difficult to interpret directly. Secondly, there might exist other explaining factors i GIS layers that might help explain the results of your experiment / inventory. ";}i:2;i:875;}i:44;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1766;}i:45;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:1766;}i:46;a:3:{i:0;s:13:"listitem_open";i:1;a:2:{i:0;i:1;i:1;i:1;}i:2;i:1766;}i:47;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:1766;}i:48;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:1;i:3;s:6:"

  - ";}i:2;i:1766;}i:49;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:84:"I will attempt to visualize some of the uncertainty in the models. The idea is this:";}i:2;i:1772;}i:50;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:3;i:3;s:84:"I will attempt to visualize some of the uncertainty in the models. The idea is this:";}i:2;i:1772;}i:51;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:1856;}i:52;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:1856;}i:53;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:1856;}i:54;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:1856;}i:55;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:2;i:3;s:9:"
      * ";}i:2;i:1856;}i:56;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:88:"If the world works as your model says it does, what will the world look like on average?";}i:2;i:1865;}i:57;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:3;i:3;s:88:"If the world works as your model says it does, what will the world look like on average?";}i:2;i:1865;}i:58;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:1953;}i:59;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:1953;}i:60;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:1953;}i:61;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:45:" What will the world look like any given day?";}i:2;i:1962;}i:62;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2007;}i:63;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2007;}i:64;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:3;}i:2;i:2007;}i:65;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2007;}i:66;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:188:" What will the world look like if it behaves a little differently every day? IN OTHER WORDS: What will the world look like if you are not a hundred percent certain of your model estimates?";}i:2;i:2015;}i:67;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2203;}i:68;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2203;}i:69;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:2203;}i:70;a:3:{i:0;s:10:"listo_open";i:1;a:0:{}i:2;i:2203;}i:71;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:3;}i:2;i:2203;}i:72;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2203;}i:73;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:75:" I will practice the art of extracting predictor variables from GIS layers.";}i:2;i:2207;}i:74;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2282;}i:75;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2282;}i:76;a:3:{i:0;s:11:"listo_close";i:1;a:0:{}i:2;i:2282;}i:77;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:2282;}i:78;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:3;}i:2;i:2282;}i:79;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2282;}i:80;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:124:" This will be done by matching the GIS layers with the inventoried coordinates and extracting the corresponding information.";}i:2;i:2290;}i:81;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2414;}i:82;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2414;}i:83;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:2414;}i:84;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2415;}i:85;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2415;}i:86;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:2415;}i:87;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2415;}i:88;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:2415;}i:89;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:4;i:3;s:1:"
";}i:2;i:2415;}i:90;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2415;}i:91;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:232:"The modelling will be quite coarse since I only have a few explanatory variables. The uncertainty of the predictions will likely be high. This is not an exercise in building a good ecological model, just to practice using the tools.";}i:2;i:2417;}i:92;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2649;}i:93;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2649;}i:94;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:66:"My aim is to automate the work as much as possible using scripts. ";}i:2;i:2651;}i:95;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2717;}i:96;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2719;}i:97;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:8:"METADATA";i:1;i:2;i:2;i:2719;}i:2;i:2719;}i:98;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:2719;}i:99;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2740;}i:100;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:11:"Raster data";i:1;i:3;i:2;i:2740;}i:2;i:2740;}i:101;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:2740;}i:102;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2740;}i:103;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:84:"Dowloaded Elevation maps in 30 meter resolution. This comes in EPSG:4326 projection.";}i:2;i:2762;}i:104;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2847;}i:105;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2847;}i:106;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:11:"Vector Data";i:1;i:3;i:2;i:2847;}i:2;i:2847;}i:107;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:2847;}i:108;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2847;}i:109;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:222:"Shape files split into different tiles. Contains elevation lines, land cover and a few other possibly usable layers. Together, I have 453 shape file that I want to import. (I use a loop)
The data has projection EPSG:32632.";}i:2;i:2869;}i:110;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3091;}i:111;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:3093;}i:112;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:21:"Text files and tables";i:1;i:3;i:2;i:3093;}i:2;i:3093;}i:113;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:3093;}i:114;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3093;}i:115;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:155:"Two text files of bumblebee and butterfly data with approx 1000 lines each. One other txt file with transect ID and X Y positions in appropriate UTM scale.";}i:2;i:3125;}i:116;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3280;}i:117;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:3282;}i:118;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:9:"Ortophoto";i:1;i:3;i:2;i:3282;}i:2;i:3282;}i:119;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:3282;}i:120;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3282;}i:121;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:14:"No orthophoto
";}i:2;i:3302;}i:122;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:3316;}i:123;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3318;}i:124;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:3320;}i:125;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:6:"METHOD";i:1;i:2;i:2;i:3320;}i:2;i:3320;}i:126;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:3320;}i:127;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:3339;}i:128;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:8:"WORKFLOW";i:1;i:3;i:2;i:3339;}i:2;i:3339;}i:129;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:3339;}i:130;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:3357;}i:131;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:3357;}i:132;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3357;}i:133;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:28:" Compile Inventory data in R";}i:2;i:3361;}i:134;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3389;}i:135;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3389;}i:136;a:3:{i:0;s:13:"listitem_open";i:1;a:2:{i:0;i:1;i:1;i:1;}i:2;i:3389;}i:137;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3389;}i:138;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:26:" Import GIS files in GRASS";}i:2;i:3393;}i:139;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3419;}i:140;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:3419;}i:141;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:3419;}i:142;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3419;}i:143;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:49:" Automate this because there are many shape files";}i:2;i:3425;}i:144;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3474;}i:145;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3474;}i:146;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:3474;}i:147;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3474;}i:148;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:49:" Create new vector point map of inventoried sites";}i:2;i:3480;}i:149;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3529;}i:150;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3529;}i:151;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:3529;}i:152;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3529;}i:153;a:3:{i:0;s:13:"listitem_open";i:1;a:2:{i:0;i:1;i:1;i:1;}i:2;i:3529;}i:154;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3529;}i:155;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:41:" Merge regions in GRASS to combined layer";}i:2;i:3533;}i:156;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3574;}i:157;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:3574;}i:158;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:3574;}i:159;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3574;}i:160;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:19:" Create new mapset ";}i:2;i:3580;}i:161;a:3:{i:0;s:18:"doublequoteopening";i:1;a:0:{}i:2;i:3599;}i:162;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:8:"Combined";}i:2;i:3600;}i:163;a:3:{i:0;s:18:"doublequoteclosing";i:1;a:0:{}i:2;i:3608;}i:164;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3609;}i:165;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3609;}i:166;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:3609;}i:167;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3609;}i:168;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:37:" Patch the different regions together";}i:2;i:3615;}i:169;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3652;}i:170;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3652;}i:171;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:3652;}i:172;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3652;}i:173;a:3:{i:0;s:13:"listitem_open";i:1;a:2:{i:0;i:1;i:1;i:1;}i:2;i:3652;}i:174;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3652;}i:175;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:57:" Extract explanatory data from GRASS at sampled locations";}i:2;i:3656;}i:176;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3713;}i:177;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:3713;}i:178;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:3713;}i:179;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3713;}i:180;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:59:" Match inventory vector points with values in other layers ";}i:2;i:3719;}i:181;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3778;}i:182;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3778;}i:183;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:3778;}i:184;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3778;}i:185;a:3:{i:0;s:13:"listitem_open";i:1;a:2:{i:0;i:1;i:1;i:1;}i:2;i:3778;}i:186;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3778;}i:187;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:38:" Export the explanatory variables to R";}i:2;i:3782;}i:188;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3820;}i:189;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:3820;}i:190;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:3820;}i:191;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3820;}i:192;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:25:" Use the spgrass6 package";}i:2;i:3826;}i:193;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3851;}i:194;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3851;}i:195;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:3851;}i:196;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3851;}i:197;a:3:{i:0;s:13:"listitem_open";i:1;a:2:{i:0;i:1;i:1;i:1;}i:2;i:3851;}i:198;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3851;}i:199;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:41:" Model the species richness and abundance";}i:2;i:3855;}i:200;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3896;}i:201;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:3896;}i:202;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:3896;}i:203;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3896;}i:204;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:16:" Do a simple GLM";}i:2;i:3902;}i:205;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3918;}i:206;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3918;}i:207;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:3918;}i:208;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3918;}i:209;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:81:" Mimic the GLM in BUGS language to get (proper?) variation in parameter estimates";}i:2;i:3924;}i:210;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:4005;}i:211;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:4005;}i:212;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:4005;}i:213;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:4005;}i:214;a:3:{i:0;s:13:"listitem_open";i:1;a:2:{i:0;i:1;i:1;i:1;}i:2;i:4005;}i:215;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:4005;}i:216;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:21:" Predict to new areas";}i:2;i:4009;}i:217;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:4030;}i:218;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:4030;}i:219;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:4030;}i:220;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:4030;}i:221;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:44:" Predict the best estimates from both models";}i:2;i:4036;}i:222;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:4080;}i:223;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:4080;}i:224;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:4080;}i:225;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:4080;}i:226;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:79:" Predict realized outcome of best estimate (What you can observe any given day)";}i:2;i:4086;}i:227;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:4165;}i:228;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:4165;}i:229;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:4165;}i:230;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:4165;}i:231;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:147:" Predict realized outcomes from a set of models, with parameter estimates varying according to the uncertainty in your knowledge of the parameters.";}i:2;i:4171;}i:232;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:4318;}i:233;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:4318;}i:234;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:4318;}i:235;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:4318;}i:236;a:3:{i:0;s:13:"listitem_open";i:1;a:2:{i:0;i:1;i:1;i:1;}i:2;i:4318;}i:237;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:4318;}i:238;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:21:" Plot the predictions";}i:2;i:4322;}i:239;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:4343;}i:240;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:4343;}i:241;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:4343;}i:242;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:4343;}i:243;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:21:" Make output pictures";}i:2;i:4349;}i:244;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:4370;}i:245;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:4370;}i:246;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:4370;}i:247;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:4370;}i:248;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:61:" Make movies of the combination of several possible outcomes.";}i:2;i:4376;}i:249;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:4437;}i:250;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:4437;}i:251;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:4437;}i:252;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:4437;}i:253;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:4437;}i:254;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4437;}i:255;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:4438;}i:256;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4440;}i:257;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:4442;}i:258;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:26:"DATA IMPORT and Processing";i:1;i:2;i:2;i:4442;}i:2;i:4442;}i:259;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:4442;}i:260;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:4481;}i:261;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:5:"GRASS";i:1;i:3;i:2;i:4481;}i:2;i:4481;}i:262;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:4481;}i:263;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:3853:"
##########################################################################
#!/bin/bash

#
#
#	This is a bash file to import all of the shape files in the Norge data set.
#	The files are located in a tree, one branch looks like this:
#	/home/astrom/Ndata/07_Vestfold/0728_Lardal/UTM32_Euref89/Shape/728_AdminOmrader_lin.shp
#
#	This means I have to travel down each branch of the tree before I can import the files one by one.
#	Grass does not like numbers in the beginning of layer names so that means I cannot just use the input name as output name.
#
#	Therefore I use grep to extract certain parts of the file name and build up a new one. The numbers in the file name correspond to the area 
#	so I have to replace the numbers with the corresponding name. Change the leading "~/Ndata/*/*/" and the "ls */*/*.shp" to suit your needs.
#
#	Author: Jens Åström 2010-07-02
#
 
 
for path in `ls -d ~/Ndata/*/*/` 
  do 
   base=`basename $path`
   newname=`echo $base |grep -o '[a-zA-Z]*'`
   cd $path
    for file in `ls */*/*.shp` 
      do 
      filebase=`basename $file | grep -o '_[a-zA-Z]*_[a-zA-Z]*'`
      infile=$path$file
      #echo $newname
      #echo $filebase
      outfile=$newname$filebase
      #echo $outfile
      #echo $file
      v.in.ogr dsn=$file out=$outfile
 
    done
  done
  
#########################################################################

#########################################################################
#!/bin/bash

#
#	This script merges the layers from different regions into one (for each type of layer) and places the new layers in the mapset "Combined"
#	In total, there is 455 vector layers that are to be combined into 15
#	Some of these layers represent region boundaries so they will be deleted later
#
#
#	Author:Jens Åström 2010-07-04
#
#	


g.mapset -c Combined ##create a new mapset
#g.mapset PERMANENT ##move back to the permanent mapset where all the different regions are located.



out=`ls ~/grassdata/Norge/PERMANENT/vector/ |grep -E -o '[a-zA-Z]*_[a-z]{3}$' |sort |uniq |grep -v 'transects_pnt'` 
for ((i=1;i<=`echo $out |wc -w`;i++)) ;do
out_now=`echo $out | awk '{print $'$i'}'`
#echo $out_now
string="ls ~/grassdata/Norge/PERMANENT/vector/ |grep `echo $out_now`" 
temp=`eval $string`
in_temp=`echo $temp |sed "s/ /@PERMANENT,/g"`
in=$in_temp@PERMANENT
#echo $in
v.patch input=$in output=$out_now -e --overwrite
done

#	This generates following warnings!
#
##	Intersections at borders will have to be snapped
##	Lines common between files will have to be edited
##	The header information also may have to be edited
#
#
#


############################################################################

############################################################################
#!/bin/bash
#
#
#	This script changes the Projection of the downloaded elevation map from EPSG:4326 to that of the Norway data, EPSG:25832 and combines the into one big tile
#
#	Then it imports this big tif in GRASS, and makes a "cut" at the extent of my other data. No point having a larger elevation map than the rest of the data.
#       
#	Finally, it copies the new raster to the mapset Combined, where all the combined tiles are
#
#
cd ~/Ndata #this is where I want the output file
rm ASTGTM_dem.tif

gdalwarp   -tr 25.0 25.0 -t_srs EPSG:32632  -s_srs EPSG:4326  ~/Ndata/ASTGTM/*/AST*dem.tif ASTGTM_dem.tif

g.mapset PERMANENT

r.in.gdal in=ASTGTM_dem.tif out=ASTGTM_dem --overwrite

g.region vect=AdminOmrader_pol@Combined nsres=25 ewres=25 -sp ##change the default region extent and resolution

r.mapcalc Elev_dem=ASTGTM_dem



g.mapset Combined
g.copy rast=Elev_dem@PERMANENT,Elev_dem

g.mapset PERMANENT
g.remove rast=Elev_dem
###############################################################################

###############################################################################

";i:1;s:4:"bash";i:2;N;}i:2;i:4502;}i:264;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4502;}i:265;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:8370;}i:266;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:8372;}i:267;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:8375;}i:268;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:1:"R";i:1;i:3;i:2;i:8375;}i:2;i:8375;}i:269;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:8375;}i:270;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:5582:"

####################################################################################

###################################################
# Import the habitat quality data
# In the habitat data file I received, the different land use types had a separate column. Here I combine them into one column. 
###################################################
setwd("~/Till AD!/GIS-Norge/R/")
habitat<-read.table("../Sandra-data/habitat.txt",header=T)
habitat$hab[habitat$open.forest==1]<-"open.forest"
habitat$hab[habitat$open.grass==1]<-"open.grass"
habitat$hab[habitat$wetlands==1]<-"wetlands"
habitat<-within(habitat,rm(list=c("open.forest","open.grass","wetlands")))

##################################################################################



##################################################################################

##################################################
# Import the transects
##################################################
## A bit confusingely, the "label" categories does not mean the same thing in the bumblebee&butterfly data as in the transects. 
## This is because those were visited three times but I only have transect data for one time (because the places does not move between visit time)
## Therefore I have to fix this. This code is just to fix the pequliarities of my data set
## This has to be run in GRASS===>>R
##################################################
library(spgrass6)
load("~/Ndata/transects.rdata")

transects$odd<-rep(c(1,0),nrow(transects)/2)
##Get rid of the end points of the transects! (both start and end points were recorded, start point will define the location of the transects)
transects<-subset(transects,transects$odd==1) 
rownames(transects@data)<-1:nrow(transects) ##Update rownames to avoid future confusion
##This next bit will split the labels, multiply everything 3 times, add the 3 time codes and finally reassemble tha labels. 
##Another option would have been to do this at the start, before matching the positions with explanatory data in GRASS

transects$label<-as.character(transects$label)
splitted<-matrix(unlist(strsplit(transects$label,"-")),nrow=nrow(transects),byrow=T)
transects$square<-splitted[,1]
transects$corner<-splitted[,2]
transects$transect<-splitted[,3]
transects<-rbind(transects,transects,transects) ##Create 3 copies, now the row numbers match with the inventory data
transects$period<-rep(1:3,each=nrow(transects)/3)
transects@data$label<-paste(transects@data[,"square"],transects@data[,"period"],transects@data[,"corner"],transects@data[,"transect"],sep="-")
transects<-transects[,c("label","Arealdekke","Elev")]
transects$label<-as.character(transects$label)

##############################################################################################################



##############################################################################################################

###################################################
# Import and set up the bumblebee data
###################################################

setwd("~/Till AD!/GIS-Norge/R/")
bumble<-read.table("../Sandra-data/humlor.txt",header=T)
#str(bumble)
temp<-bumble[,7:58] ##Subsample only the species column for further manipulation
all.names<-gsub("(^B\\.[a-z]*)(\\..)","\\1",names(temp)) ##Remove trailing letters indicating caste and sex of individuals
unique.names<-unique(all.names) ##With caste and sex info taken away, we have multiple occurrences of same name. Take only 1 of each
new<-data.frame(matrix(nrow=nrow(temp),ncol=length(unique.names)))##Make an empty temporary data frame 
names(new)<-unique.names ##Assign names, and fill it with the summed abundances at species level. This will contain all the correct abundance data
for(i in 1:length(unique.names)){
new[,i]<-rowSums(temp[all.names==unique.names[i]])
}
bumble<-within(bumble,rm(list=c(names(bumble)[7:58]))) ##Get rid of old data
bumble<-cbind(bumble,new) ##replace with new data (at species level)
rm(temp) ##A little housekeeping
rm(new)
bumble$spec.rich<-rowSums(bumble[,8:22]>0) ##Create a new column for species richness
bumble<-merge(habitat,bumble) ##Merge the habitat quality data frame with the inventory results,  and set the appropriate explanatory variables as factors
for(i in 1:7){
bumble[,i]<-as.factor(bumble[,i])
}
##Add combined transect ID string for future merging with transect location data
bumble$label<-paste(bumble[,"square"],bumble[,"period"],bumble[,"corner"],bumble[,"transect"],sep="-") 
bumble<-merge(transects,bumble)
bumble$flower.cov<-as.numeric(bumble$flower.cov) ##Set the flower cover variable as numeric (Can be questioned, maybe it is a ordinal category?)


##################################################################################

#####################################################################################################################

###################################################
# Reshaping data
# THIS bit is not necessary when working on all of the data, only when I want to aggregate on some level, e.g. period.
# I include the code anyhow, since reshape is very useful.The point is to aggregate the data at some level.
# For example aggregate data censored at the village level to county or region level.
###################################################

bumble.melt<-melt(bumble,c(1:13))
bumble<-cast(bumble.melt,formula=square+period+corner+transect+Sandra.JanOve+hab+flower.cov+Arealdekke+Elev~variable,sum)

#####################################################################################################################


";i:1;s:1:"r";i:2;N;}i:2;i:8392;}i:271;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:8392;}i:272;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:13985;}i:273;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:13987;}i:274;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:13988;}i:275;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:13988;}i:276;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:32:"Get new information out of GRASS";i:1;i:2;i:2;i:13988;}i:2;i:13988;}i:277;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:13988;}i:278;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:1610:"
#!/bin/bash
#
#
#	This script queries the Land cover layer and the Elevation layer for their information at the locations of the visited transects.
#
#	This information will be used to model the results of the inventory.
#
#
#
#
#

g.mapset Combined
g.region -dp

#g.copy vect=transects_pnt@PERMANENT,transects_pnt

v.db.addcol transects_pnt col="Arealdekke varchar(20)"
v.what.vect transects_pnt qvector=Arealdekke_pol qcolumn=OBJTYPE col=Arealdekke

#v.db.select transects_pnt columns=Arealdekke

v.db.addcol transects_pnt col="Elev integer"

v.what.rast vector=transects_pnt raster=Elev_dem layer=1 column=Elev

##########################################################################
#
#
#	This script export/imports the vector layer "transects_pnt" (with the added information in it) into R
#
#
#
#
#
#
#


R

library("spgrass6")

transects<-readVECT6("transects_pnt")

## A little housekeeping, fixing the character encoding errors

transects$Arealdekke<-as.character(transects$Arealdekke)

transects@data$Arealdekke[transects@data$Arealdekke=="\xc5pentOmr\xe5de"]<-"OpentOmrade"
transects@data$Arealdekke[transects@data$Arealdekke=="Industriomr\xe5de"]<-"Industriomr"

## One of the visited places was located ~ 100 metres outside of the region I had GIS layers of.
## Looking at google maps, I could see that the land type was Forest = Skog

transects$Arealdekke[which(is.na(transects@data$Arealdekke))]<-"Skog" 
transects$Arealdekke<-as.factor(transects$Arealdekke)

save(file="~/Ndata/transects.rdata",transects)

###############################################################################
 

";i:1;N;i:2;N;}i:2;i:14039;}i:279;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:15660;}i:280;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:9:"Modelling";i:1;i:2;i:2;i:15660;}i:2;i:15660;}i:281;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:15660;}i:282;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:15683;}i:283;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:10:"IN R (GLM)";i:1;i:3;i:2;i:15683;}i:2;i:15683;}i:284;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:15683;}i:285;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:101:"

calib3<-glm(tot.huml~Arealdekke+Elev,data=bumble,family=poisson) ##Simplistic GLM to predict from

";i:1;s:1:"r";i:2;N;}i:2;i:15709;}i:286;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:15709;}i:287;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:15821;}i:288;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:15823;}i:289;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:15824;}i:290;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:15824;}i:291;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:33:"IN JAGS (Bayesian version of GLM)";i:1;i:3;i:2;i:15824;}i:2;i:15824;}i:292;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:15824;}i:293;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:2078:"
#################################################
### Bayesian version of the simple GLM model
#################################################

##This script will produce a Bayesian version of the simple GLM I did earlier.
##The result will be that the uncertainty in all the parameter estimations will be taken into account
## when estimating each parameter. This will result in wider confidence bounds for the parameters (which can
## be said to better represent our unceartainty about the system)
##
##
## Finally, I will not just use the best estimate of all the parameters in the GLM for the predictions, but instead take random draws
## from the posterior distribution of these parameters. One set of parameter draws will be used to create one prediction about the system.
## Then, I will take another draw and repeat the process, resulting in many realized predictions.
## Most of the parameter random draws will lie close to the best estimate, but I will allow for the fact
## that I don't have absolute knowledge about the exact value of these parameters (or if you like, that there is no single "true" value)
##
## With this set of predictions, I can then visualize the uncertainty, calculate variation of predictions for each pixel or the whole map, sort them and display the 2.5 and 97.5 % quantiles 
## to create "credable confidence bounds" on the maps

##First load the "rjags" package

library(rjags) ##This will make R and JAGS talk to each other
load.module("glm") ##This is a JAGS module that is good to use when modelling GLM like models

##Set up the model in JAGS (The actual model is in the text file, shown a bit later below)
Bayesian.glm<-jags.model("~/scripts/glm1.txt",data=bumble,n.chains=3)
update(Bayesian.glm,1000) ## Burn in period for the Marcov chains

## Get samples of the MCMC chains. Specify wich parameters to "collect" 10 000 updates and collect only every 50th to combat autocorrelation
B.out<-coda.samples(Bayesian.glm,c("intercept","dekke","elev.par"),n.iter=10000,thin=50) 

gelman.diag(B.out[,2:8]) # Check convergence of chains

";i:1;s:1:"r";i:2;N;}i:2;i:15874;}i:294;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:17964;}i:295;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:38:"And here is the actual BUGS/JAGS model";i:1;i:4;i:2;i:17964;}i:2;i:17964;}i:296;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:4;}i:2;i:17964;}i:297;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:321:"
model{

for(i in 1:length(tot.huml)){

      tot.huml[i]~dpois(est[i])
      log(est[i])<-intercept+dekke[Arealdekke[i]]+elev.par*Elev[i]
      
      }
      
      dekke[1]<-0
      for(j in 2:6){
	  dekke[j]~dnorm(0.0,1.0E-6)
	  }
      intercept~dnorm(0.0,1.0E-6)
      elev.par~dnorm(0.0,1.0E-6)
      
}
      
  
";i:1;N;i:2;N;}i:2;i:18016;}i:298;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:18348;}i:299;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:10:"Prediction";i:1;i:2;i:2;i:18348;}i:2;i:18348;}i:300;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:18348;}i:301;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:18372;}i:302;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:4:"IN R";i:1;i:3;i:2;i:18372;}i:2;i:18372;}i:303;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:18372;}i:304;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:18372;}i:305;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:18387;}i:306;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:18389;}i:307;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:18395;}i:308;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:5279:"
#################################################

#!/bin/bash
#
#	This script copies all the vectors in the PERMANENT mapset starting with Eidsberg to the new location Eidberg.
#	This subregion will be the target for prediction since predicting on the whole data set at 25 meters resolution 
#	turned out to be too much for my computer
#
#
#

g.mapset -c Eidsberg

for file in Eidsberg_AdminOmrader_lin Eidsberg_AdminOmrader_pol Eidsberg_Arealdekke_lin Eidsberg_Arealdekke_pnt Eidsberg_Arealdekke_pol Eidsberg_ByggOgAnlegg_lin Eidsberg_ByggOgAnlegg_pnt Eidsberg_ByggOgAnlegg_pol Eidsberg_Hoyde_lin Eidsberg_Hoyde_pnt Eidsberg_RestriksjonsOmrader_lin Eidsberg_RestriksjonsOmrader_pol Eidsberg_Samferdsel_lin Eidsberg_Samferdsel_pnt    
do

g.copy vect=$file@PERMANENT,$file

done

##########################################################################



################################################################################################################
#
#
#   This script first changes the land use types to numeric codes. 
#   Then it creates a raster of the land use information to get values at every pixel.
#   Then it fits a simple GLM to the data, using the newly acquired information as explanatory variables.
#   As an example, I will use the total abundance of Bumblebees.
#
#
#


R

library(spgrass6)
##This is the corresponding codes for the land use types. I have to use numbers in the models later

#1 Alpinbakke
#2 BymessigBebyggelse
#3 DyrketMark
#4 ElvBekk
#5 FerskvannTorrfall
#6 Golfbane
#7 Gravplass
#8 Havflate
#9 Industriomrade
#10 Innsjo
#11 Lufthavn
#12 Myr
#13 Park
#14 OpentOmrade
#15 Skog
#16 SportIdrettPlass
#17 Steinbrudd
#18 Steintipp
#19 TettBebyggelse

##It turned out to be a bit of a chore changing the land use names to numbers in GRASS, so I did it in R in stead


areal<-readVECT6("Arealdekke_num") ##Import the vector into R
areal@data$NR<-as.numeric(areal@data$OBJTYPE) #change it to numeric,easy-peasy
writeVECT6(areal,"areal_num") ##Export it again to GRASS

##  Then make a raster out of it to get the information on a pixel level
system("v.to.rast input=areal_num@Combined output=arealdekke_num use=attr type=point,line,area layer=1 column=NR value=1 rows=4096 ") ##Call grass with "system"


system("g.mapset Eidsberg") ## Make sure you have the correct mapset
areal<-readRAST6("Arealdekke_eid") ##This is the land use information
gc() ##This clean the memory of garbage, makes you computer happy. Useful when handling large files
elev<-readRAST6("Elev_dem_eid") ## This is the elevation information, already in raster format
gc()

pred.frame<-data.frame(c(areal@data,elev@data)) ##Make a data frame with both predictors to work on
names(pred.frame)<-c("Arealdekke","Elev")
pred.frame$Arealdekke<-factor(pred.frame$Arealdekke) ##Make it into a factor
gc()


pred.frame$Arealdekke<-as.character(pred.frame$Arealdekke)
pred.frame$Arealdekke[is.na(pred.frame$Arealdekke)]<-0 ##Weird things happen when you have NAs in the treatment categories. Replace them with zeroes
pred.frame$Arealdekke<-as.factor(pred.frame$Arealdekke)

##The next part is a bit complicated if you don't know the data. 
##It has to do with the fact that the are more land use categories in the new data set then the ones used for model calibration. 
##The following code is really ugly at some places
##First make a subset of the new region that only contains the land use types visited
temp<-pred.frame[pred.frame$Arealdekke==3 | pred.frame$Arealdekke==9 | pred.frame$Arealdekke==12 | pred.frame$Arealdekke==14 | pred.frame$Arealdekke==15 | pred.frame$Arealdekke==19,]
bla<-as.factor(as.character(temp$Arealdekke))
temp$Arealdekke<-bla
rm(bla)
#table(temp$Arealdekke) 

## Reclassify the land use types visited when inventoring to numbers
bumble$Arealdekke<-as.character(bumble$Arealdekke)
bumble$Arealdekke[bumble$Arealdekke=="DyrketMark"]<-"3"
bumble$Arealdekke[bumble$Arealdekke=="Gravplass"]<-"7"
bumble$Arealdekke[bumble$Arealdekke=="Industriomr"]<-"9"
bumble$Arealdekke[bumble$Arealdekke=="Myr"]<-"12"
bumble$Arealdekke[bumble$Arealdekke=="OpentOmrade"]<-"14"
bumble$Arealdekke[bumble$Arealdekke=="Skog"]<-"15"
bumble$Arealdekke[bumble$Arealdekke=="TettBebyggelse"]<-"19"
bumble$Arealdekke<-as.factor(bumble$Arealdekke)

##Make a prediction based on a simplistic GLM
##
#pred.frame$predict<-rep(0,nrow(pred.frame)) ##Make a new vector in the prediction data frame for the predictions. Areas without information will get value 0. This can be changed later
pred.frame$predict<-rep(NA,nrow(pred.frame))
calib3<-glm(tot.huml~Arealdekke+Elev,data=bumble,family=poisson) ##Simplistic GLM to predict from
predicted<-predict(calib3,temp,type="response") ##New prediction
#pred.frame$predict<-numeric(nrow(pred.frame))
##Fill the right slots with predictions
pred.frame$predict[pred.frame$Arealdekke==3 | pred.frame$Arealdekke==9 | pred.frame$Arealdekke==12 | pred.frame$Arealdekke==14 | pred.frame$Arealdekke==15 | pred.frame$Arealdekke==19]<-predicted
elev$predicted<-pred.frame$predict
## Prediction done!
## The prediction can then be transported to GRASS by: writeRAST6(elev,"New_raster",zcol="predict",overwrite=T)

###########################################################################################################################

";i:1;s:1:"r";i:2;N;}i:2;i:18395;}i:309;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:23686;}i:310;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:14:"Bayesian model";i:1;i:3;i:2;i:23686;}i:2;i:23686;}i:311;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:23686;}i:312;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:1721:"
B.res<-summary(B.out[,2:8])$statistics
B.est<-B.res[,1]

##Rearrange so that Intercept term is in the first slot
B.est<-c(B.est[7],B.est[1:6])

B.prediction<-exp(model.matrix(glm(1:nrow(temp)~Arealdekke+Elev,data=temp)) %*% B.est) ##This is where the prediction is made. Similar function as predict() function

cor.test(temp$B.pred,temp$predict) ##Correlation between the GLM (frequentist) estimate and the Bayesian estimate == 92%


################WRITE THE BAYESIAN PREDICTION###############
pred.frame$predict<-rep(NA,nrow(pred.frame)) ##Fill it with NAs. This is replaced by predicted values next.
pred.frame$predict[pred.frame$Arealdekke==3 | pred.frame$Arealdekke==9 | pred.frame$Arealdekke==12 | pred.frame$Arealdekke==14 | pred.frame$Arealdekke==15 | pred.frame$Arealdekke==19]<-B.prediction
elev$B.pred<-pred.frame$predict

##Done!
##

##################################################################################


#####RANDOM DRAWS FROM POSTERIOR PARAMETER DISTRIBUTIONS##############



##The function B.new draws random numbers from the output of the Bayesian analysis, by simply sampling the Coda outputs. This way, I can only draw numbers that were actually produced by JAGS.
## Another way would be to define a distribution from the estimated standard errors from the output. Then I would be able to draw any number from the distribution. 

B.new<-function(){
temp<-c(sample(size=1,unlist(B.out[,8])),sample(size=1,unlist(B.out[,2])),sample(size=1,unlist(B.out[,3])),sample(size=1,unlist(B.out[,4])),sample(size=1,unlist(B.out[,5])),sample(size=1,unlist(B.out[,6])),sample(size=1,unlist(B.out[,7])))
return(temp)
}

################################################################################

";i:1;s:1:"r";i:2;N;}i:2;i:23716;}i:313;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:25449;}i:314;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:18:"Let's make a movie";i:1;i:3;i:2;i:25449;}i:2;i:25449;}i:315;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:25449;}i:316;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:1184:"
##################################################
###make movie, Bayesian random parameter draw version
##################################################
setwd("/home/astrom/Till AD!/GIS-Norge/out")
#system("g.region rows=1080 cols=1280 -p")
system("g.region rows=1412 cols=1670 -p")
#system("g.region rast=Elev_dem_eid -p")


for(i in 1:50){
B.prediction<-exp(model.matrix(glm(1:nrow(temp)~Arealdekke+Elev,data=temp)) %*% B.new()) ##Multiply the model matrix with the random drawn parameters
pred.frame$predict<-numeric(nrow(pred.frame))
pred.frame$predict[pred.frame$Arealdekke==3 | pred.frame$Arealdekke==9 | pred.frame$Arealdekke==12 | pred.frame$Arealdekke==14 | pred.frame$Arealdekke==15 | pred.frame$Arealdekke==19]<-B.prediction
elev$B.pred<-pred.frame$predict

elev$B.rand.pois<-rpois(nrow(elev),elev$B.pred) ##Pick one random draw from the estimated mean in each pixel

writeRAST6(elev,paste("pois",i,sep=""),zcol="B.rand.pois",overwrite=T)
}
system("for i in `seq 1 50` 
 do
  r.out.png input=pois$i output=pois$i
   g.remove rast=pois$i 
    done")

system("convert -delay 20 *.png 1670B-rand-pois.mpg")


##############################################################
";i:1;s:1:"r";i:2;N;}i:2;i:25483;}i:317;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:26679;}i:318;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:10:"Validation";i:1;i:3;i:2;i:26679;}i:2;i:26679;}i:319;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:26679;}i:320;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:26679;}i:321;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:343:"I won't make any attempts at validating my models. I already know they are extremely simplistic and will not have much explanatory power. One predictor that seems to be important is Flower cover, but I don't have data for that other than at the visited sites. Interpolating these values might be a possible road but I won't look into that now.";}i:2;i:26700;}i:322;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:27043;}i:323;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:27045;}i:324;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:22:"RESULTS and DISCUSSION";i:1;i:2;i:2;i:27045;}i:2;i:27045;}i:325;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:27045;}i:326;a:3:{i:0;s:10:"listo_open";i:1;a:0:{}i:2;i:27079;}i:327;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:27079;}i:328;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:27079;}i:329;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:170:" The first picture shows the prediction from the GLM model. White areas are those with land uses the inventory personnel did not visit, so I won't predict to those areas.";}i:2;i:27083;}i:330;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:27253;}i:331;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:27253;}i:332;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:27253;}i:333;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:27253;}i:334;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:172:" The second picture shows the corresponding output for the Bayesian analysis. The parameter values changes slightly which creates a pretty large change in the colormapping.";}i:2;i:27257;}i:335;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:27429;}i:336;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:27429;}i:337;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:27429;}i:338;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:27429;}i:339;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:183:" The third picture shows the best estimate frequentistic GLM (1) prediction but for one random outcome. That is I draw a random number from a Poisson dist with the prediction as mean.";}i:2;i:27433;}i:340;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:27616;}i:341;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:27616;}i:342;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:27616;}i:343;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:27616;}i:344;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:279:" The third picture/movie show 10 possible outcomes when you let the parameter values vary randomly according to the uncertainty in the Bayesian estimation, plus random noise as in (3). You can make a longer movie as well with better resolution but it won't fit in this wiki-page ";}i:2;i:27620;}i:345;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:27899;}i:346;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:27899;}i:347;a:3:{i:0;s:11:"listo_close";i:1;a:0:{}i:2;i:27899;}i:348;a:3:{i:0;s:10:"listo_open";i:1;a:0:{}i:2;i:27900;}i:349;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:27900;}i:350;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:27900;}i:351;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:163:" The fourth plot shows a diagnostic plot of the Bayesian estimation. The MCMC chains seems to have converged well, and the parameter distributions look reasonable.";}i:2;i:27904;}i:352;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:28067;}i:353;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:28067;}i:354;a:3:{i:0;s:11:"listo_close";i:1;a:0:{}i:2;i:28067;}i:355;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:28067;}i:356;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:28068;}i:357;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:28070;}i:358;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:21:":wikidk:f_pred_na.png";i:1;s:37:"Frequentist prediction, best estimate";i:2;N;i:3;s:3:"400";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:28071;}i:359;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:28138;}i:360;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:21:":wikidk:b_pred_na.png";i:1;s:34:"Bayesian prediction, best estimate";i:2;N;i:3;s:3:"400";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:28139;}i:361;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:28203;}i:362;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:32:":wikidk:f_pred_pois_na_small.png";i:1;s:60:"Frequentist prediction, best estimate + poisson random draws";i:2;N;i:3;s:3:"400";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:28204;}i:363;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:28305;}i:364;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:25:":wikidk:b_par_pois_na.gif";i:1;s:116:"10 random outcomes of bumblebees. Random parameter draws + poisson random draws. Bigger movie is problematic on wiki";i:2;N;i:3;N;i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:28306;}i:365;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:28452;}i:366;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:20:":wikidk:mcmcplot.png";i:1;s:36:"Diagnostic plot of the MCMC sampling";i:2;N;i:3;s:3:"400";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:28453;}i:367;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:28518;}i:368;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:28519;}i:369;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:28521;}i:370;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:28521;}i:371;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:28521;}i:372;a:3:{i:0;s:12:"document_end";i:1;a:0:{}i:2;i:28521;}}