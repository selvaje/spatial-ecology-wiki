a:391:{i:0;a:3:{i:0;s:14:"document_start";i:1;a:0:{}i:2;i:0;}i:1;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:61:"Habitat Suitability and uncertainty Modeling of Mollusk Fauna";i:1;i:1;i:2;i:1;}i:2;i:1;}i:2;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:1;}i:2;i:1;}i:3;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1;}i:4;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:77;}i:5;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:79;}i:6;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:79;}i:7;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:79;}i:8;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:79;}i:9;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:31:" Download the project proposal ";}i:2;i:83;}i:10;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:92:"http://www.spatial-ecology.net/ost4sem/project/unidk2010/MARINE/documents/Copenhagen2010.pdf";i:1;s:21:"  Copenhagen2010.pdf ";}i:2;i:114;}i:11;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:".";}i:2;i:234;}i:12;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:235;}i:13;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:235;}i:14;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:235;}i:15;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:235;}i:16;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:236;}i:17;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:238;}i:18;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:239;}i:19;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:239;}i:20;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:12:"INTRODUCTION";i:1;i:2;i:2;i:239;}i:2;i:239;}i:21;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:239;}i:22;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:239;}i:23;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:265;}i:24;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:32:" I work together with Jens, see ";}i:2;i:267;}i:25;a:3:{i:0;s:12:"internallink";i:1;a:2:{i:0;s:8:"dk10mbee";i:1;s:7:" MODB&B";}i:2;i:299;}i:26;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:79:". We use basically the same workflow and methods, but with different data sets.";}i:2;i:320;}i:27;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:399;}i:28;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:401;}i:29;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:403;}i:30;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:34:"General framework of this analysis";i:1;i:3;i:2;i:403;}i:2;i:403;}i:31;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:403;}i:32;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:403;}i:33;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:796:"There are many methods available within the framework of Habitat Suitability Modeling (HMS). Common to all of them is that there will be a varying degree of uncertainty related to all of them depending on the variance related to the fitted values. This uncertainty is a key issue in HMS and needs to be addressed if we want our predictions to be valid in and useful to management and decision-makers. Jens and I address a simple method to visualize the uncertainty inherent to a particular prediction. This method is based on a random pick of drawn from our predictions based on the functions rnorm and rpois in R. These n random picks, based on the mean and SD of the fitted values of the model, can be considered as what would be likely to observe in the field if we went to each pixel n times.";}i:2;i:449;}i:34;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1245;}i:35;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:1247;}i:36;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:18:"Project objectives";i:1;i:3;i:2;i:1247;}i:2;i:1247;}i:37;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:1247;}i:38;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1247;}i:39;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:54:"I had the following goals of this last week of coding:";}i:2;i:1277;}i:40;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1331;}i:41;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1331;}i:42;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:108:"1) Using, or at least touching, as many as possible of the tools taught during the first week of the course.";}i:2;i:1333;}i:43;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1441;}i:44;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:1441;}i:45;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:1441;}i:46;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:1441;}i:47;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:1;i:3;s:6:"

  * ";}i:2;i:1441;}i:48;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:4:"Bash";}i:2;i:1447;}i:49;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:3;i:3;s:4:"Bash";}i:2;i:1447;}i:50;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:1451;}i:51;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:1451;}i:52;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:1451;}i:53;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:4:" AWK";}i:2;i:1456;}i:54;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:1460;}i:55;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:1460;}i:56;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:1460;}i:57;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:1460;}i:58;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:24:" Bash-scripting of GRASS";}i:2;i:1464;}i:59;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:1488;}i:60;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:1488;}i:61;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:1488;}i:62;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:1488;}i:63;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:13:" GRASS & Qgis";}i:2;i:1492;}i:64;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:1505;}i:65;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:1505;}i:66;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:1505;}i:67;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:1505;}i:68;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:2:" R";}i:2;i:1509;}i:69;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:1511;}i:70;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:1511;}i:71;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:1511;}i:72;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:1511;}i:73;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:12:" ImageMagick";}i:2;i:1515;}i:74;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:1527;}i:75;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:1527;}i:76;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:1527;}i:77;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1527;}i:78;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:65:"2) Avoid manual editing of files in spreadsheets or text editors.";}i:2;i:1529;}i:79;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:3;i:3;s:65:"2) Avoid manual editing of files in spreadsheets or text editors.";}i:2;i:1529;}i:80;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1594;}i:81;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:1594;}i:82;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:1594;}i:83;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:1594;}i:84;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:4;i:3;s:1:"
";}i:2;i:1594;}i:85;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1594;}i:86;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:38:"3) Looping over repetitive operations.";}i:2;i:1596;}i:87;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1634;}i:88;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1634;}i:89;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:58:"4) Make the code as general as possible (still a way to go";}i:2;i:1636;}i:90;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:3:"...";}i:2;i:1694;}i:91;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:2:").";}i:2;i:1697;}i:92;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1699;}i:93;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:1699;}i:94;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:1699;}i:95;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:1699;}i:96;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:57:" This would allow for creating functions (at least in R).";}i:2;i:1703;}i:97;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:1760;}i:98;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:1760;}i:99;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:1760;}i:100;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1760;}i:101;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:75:"5) Prepare a species data set and a predictor data set and merge/join them.";}i:2;i:1762;}i:102;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1837;}i:103;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1837;}i:104;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:51:"6) Fit a GLM and GAM model to the Mollusk richness.";}i:2;i:1839;}i:105;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1890;}i:106;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1890;}i:107;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:58:"7) Predict both models to new data (i.e. unsampled areas).";}i:2;i:1892;}i:108;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1950;}i:109;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1950;}i:110;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:92:"8) Model uncertainty in the models by drawing random values from the fitted functions (i.e. ";}i:2;i:1952;}i:111;a:3:{i:0;s:18:"doublequoteopening";i:1;a:0:{}i:2;i:2044;}i:112;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:14:"new fieldtrips";}i:2;i:2045;}i:113;a:3:{i:0;s:18:"doublequoteclosing";i:1;a:0:{}i:2;i:2059;}i:114;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:2:").";}i:2;i:2060;}i:115;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2062;}i:116;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2062;}i:117;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:90:"9) Visualize uncertainty by making movies of predicted outcomes of the uncertainty models.";}i:2;i:2064;}i:118;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2154;}i:119;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2154;}i:120;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:2160;}i:121;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:2162;}i:122;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2163;}i:123;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2163;}i:124;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:8:"METADATA";i:1;i:2;i:2;i:2163;}i:2;i:2163;}i:125;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:2163;}i:126;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2184;}i:127;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:11:"Raster data";i:1;i:3;i:2;i:2184;}i:2;i:2184;}i:128;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:2184;}i:129;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:2205;}i:130;a:3:{i:0;s:13:"listitem_open";i:1;a:2:{i:0;i:1;i:1;i:1;}i:2;i:2205;}i:131;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2205;}i:132;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:17:" Bathymetry (DEM)";}i:2;i:2209;}i:133;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2226;}i:134;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:2226;}i:135;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:2226;}i:136;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2226;}i:137;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:37:" Res = 1 m, Extension = 1000 x 1050 m";}i:2;i:2232;}i:138;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2269;}i:139;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2269;}i:140;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:2269;}i:141;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2269;}i:142;a:3:{i:0;s:13:"listitem_open";i:1;a:2:{i:0;i:1;i:1;i:1;}i:2;i:2269;}i:143;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2269;}i:144;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:35:" Terrain variables derived from DEM";}i:2;i:2273;}i:145;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2308;}i:146;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:2308;}i:147;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:2308;}i:148;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2308;}i:149;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:30:" BPI, slope, aspect, curvature";}i:2;i:2314;}i:150;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2344;}i:151;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2344;}i:152;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:2344;}i:153;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2344;}i:154;a:3:{i:0;s:13:"listitem_open";i:1;a:2:{i:0;i:1;i:1;i:1;}i:2;i:2344;}i:155;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2344;}i:156;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:53:" Sea current data - bottom, surface, and water column";}i:2;i:2348;}i:157;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2401;}i:158;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:2401;}i:159;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:2;}i:2;i:2401;}i:160;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2401;}i:161;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:" res = 15 m";}i:2;i:2407;}i:162;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2418;}i:163;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2418;}i:164;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:2418;}i:165;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2418;}i:166;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:2418;}i:167;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2418;}i:168;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:41:" Acoustic classification from single-beam";}i:2;i:2422;}i:169;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2463;}i:170;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2463;}i:171;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:2463;}i:172;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2463;}i:173;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:47:" Habitat model (based on BPI, slope, and depth)";}i:2;i:2467;}i:174;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2514;}i:175;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2514;}i:176;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:2514;}i:177;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2515;}i:178;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:11:"Vector Data";i:1;i:3;i:2;i:2515;}i:2;i:2515;}i:179;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:2515;}i:180;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:2536;}i:181;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:2536;}i:182;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2536;}i:183;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:32:" Stations - 28 stations in total";}i:2;i:2540;}i:184;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2572;}i:185;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2572;}i:186;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:2572;}i:187;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2573;}i:188;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:21:"Text files and tables";i:1;i:3;i:2;i:2573;}i:2;i:2573;}i:189;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:2573;}i:190;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:2604;}i:191;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:2604;}i:192;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2604;}i:193;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:21:" Grainsize (sediment)";}i:2;i:2608;}i:194;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2629;}i:195;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2629;}i:196;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:2629;}i:197;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2629;}i:198;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:25:" Species-Abundance Matrix";}i:2;i:2633;}i:199;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2658;}i:200;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2658;}i:201;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:2658;}i:202;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2658;}i:203;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:17:" Predictor Matrix";}i:2;i:2662;}i:204;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2679;}i:205;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2679;}i:206;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:2679;}i:207;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2680;}i:208;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:9:"Ortophoto";i:1;i:3;i:2;i:2680;}i:2;i:2680;}i:209;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:2680;}i:210;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2680;}i:211;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:2700;}i:212;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:2702;}i:213;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2703;}i:214;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2703;}i:215;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:6:"METHOD";i:1;i:2;i:2;i:2703;}i:2;i:2703;}i:216;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:2703;}i:217;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2722;}i:218;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:8:"WORKFLOW";i:1;i:3;i:2;i:2722;}i:2;i:2722;}i:219;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:2722;}i:220;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:2740;}i:221;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:2740;}i:222;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2740;}i:223;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:90:" Preparing two predictor datasets (EnvMat.txt and EnvMat2.csv) using bash, awk, and GRASS.";}i:2;i:2744;}i:224;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2834;}i:225;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2834;}i:226;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:2834;}i:227;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:2835;}i:228;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:2835;}i:229;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2835;}i:230;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:56:" Merging species data set with predictor data sets in R.";}i:2;i:2839;}i:231;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2895;}i:232;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2895;}i:233;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:2895;}i:234;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:2896;}i:235;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:2896;}i:236;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2896;}i:237;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:101:" Fitting GLM and GAM models in R using mollusk richness per sampled station as the response variable.";}i:2;i:2900;}i:238;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3001;}i:239;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3001;}i:240;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:3001;}i:241;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:3002;}i:242;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:3002;}i:243;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3002;}i:244;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:78:" Prepared a data frame holding the new data to which I want to predict (in R).";}i:2;i:3006;}i:245;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3084;}i:246;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3084;}i:247;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:3084;}i:248;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:3085;}i:249;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:3085;}i:250;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3085;}i:251;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:68:" Predicting to new data/unvisited areas for both GLM and GAM models.";}i:2;i:3089;}i:252;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3157;}i:253;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3157;}i:254;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:3157;}i:255;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:3158;}i:256;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:3158;}i:257;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3158;}i:258;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:140:" In an attempt to estimate uncertainty of our models, Jens and I were using the rnorm function in R to simulate the possible outcome of new ";}i:2;i:3162;}i:259;a:3:{i:0;s:18:"doublequoteopening";i:1;a:0:{}i:2;i:3302;}i:260;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"field trips";}i:2;i:3303;}i:261;a:3:{i:0;s:18:"doublequoteclosing";i:1;a:0:{}i:2;i:3314;}i:262;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:".";}i:2;i:3315;}i:263;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3316;}i:264;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3316;}i:265;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:3316;}i:266;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:3317;}i:267;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:3317;}i:268;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:3317;}i:269;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:68:" Making an animation to visualize the uncertainty discovered by the ";}i:2;i:3321;}i:270;a:3:{i:0;s:18:"doublequoteopening";i:1;a:0:{}i:2;i:3389;}i:271;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"field trips";}i:2;i:3390;}i:272;a:3:{i:0;s:18:"doublequoteclosing";i:1;a:0:{}i:2;i:3401;}i:273;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3402;}i:274;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3402;}i:275;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:3402;}i:276;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3402;}i:277;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:3404;}i:278;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:3406;}i:279;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3407;}i:280;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:3407;}i:281;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:11:"DATA IMPORT";i:1;i:2;i:2;i:3407;}i:2;i:3407;}i:282;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:3407;}i:283;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:14680:"
### Habitat Suitability and uncertainty Modeling of Mollusk Fauna
### Thijs Chr. van Son (Jens Åstrøm)
 
INDIR=~/a/ost4sem/input			# setting the input file directory 
OUTDIR=~/a/ost4sem/output 		# setting the output file directory 
 
### Extracting data from Grainsize.csv and put it into EnvMat.txt
cd $INDIR
# awk '{ print NF }' EnvMat.csv		# there are 8 columns 

#######################################################################
#######################################################################
################ PREPARATION OF PREDICTOR DATA SET ####################
#######################################################################
#######################################################################

#######################################################################
############## Extracting grainsize parameters in R ###############
#######################################################################

### For each Station:
## - linearly interpolate between known grainsize points
## - extract percentiles
## - calculate grainsize parameters
## - load percentiles and parameters in a matrices

# Importing the grainsize sediment data
Grainsize <- read.csv("/home/thijs/a/PhD_data/Fielddata/EnvData/stut_grsz_compl.csv")
str(Grainsize)
head(Grainsize)

## Fixing a typo
Grainsize$dry.wt <- Grainsize$dry.weigth
str(Grainsize)

## Extracting the 0-2 cm layer
Upper <- Grainsize[Grainsize$layer == "0-2", ] # Here the subset function can be used also
str(Upper)
head(Upper)

## Creating a matrix to hold all parameters for each replicate
AllReps <- unique(Upper$sta.rep)
a <- length(AllReps)
Params <- c("Median", "Sorting", "Skewness", "Kurtosis")
Param.mat <- matrix(data = NA, nrow = length(AllReps), ncol = length(Params))

## Defining objects to be used in Function(dt, percV) and
## creates a matrix to hold percentiles for each replicateubuntu
percV <- c(5, 16, 25, 50, 75, 84, 95)
o <- length(percV)
dt <- Upper
Perc.mat <- matrix(data = NA, nrow = length(AllReps), ncol = length(percV))

## Sourcing my functions
source("/home/thijs/a/R/RCode/my_functions.r")

## Run the PPsLog-function in a loop over all stations
for (i in seq(length = a)) {
    Rep.i <- AllReps[i]
    dt.i <- dt[dt$sta.rep == Rep.i, ]
    # output <- PPsGeo(dt.i, percV)
    output <- PPsLog(dt.i, percV)
    Param.mat[i, ] <- output$a
    Perc.mat[i, ] <- output$b
    }

## Setting the colnames and rownames of Param.mat
colnames(Param.mat) <- Params
rownames(Param.mat) <- seq(1:35)
Param.mat

## Coerce the matrix into a data frame  
Param.mat.df <- as.data.frame(Param.mat)
str(Param.mat.df)

## Adding the stations to the data frame
Param.mat.df$Reps <- AllReps

## Selecting Stations, Median,and Sorting
Param.mat.df <- subset(Param.mat.df, select = c("Reps", "Median", "Sorting"))

## Stringsplit of the Reps column, which contain  both station and replicate in one string (e.g. 1_1)
# The following splits the Reps column in two, and splitting at the underscore ("_")
Rep_split <- strsplit(as.character(Param.mat.df$Reps), "_")	# Creates a list of splitted values

## Unlist the list created above and convert it into a matrix 
Rep_split <- matrix(unlist(Rep_split), ncol=2, byrow = TRUE)

## Adding the Rep_split outcome to Param.mat.df
Param.mat.df$Station <- Rep_split[, 1]
Param.mat.df$Replicate <- Rep_split[, 2]

## Reshaping the data
# Reshape can be used to apply functions within a variable containing subvariables
# For example, I had some stations that had 3 replicates, and I wanted the mean of those
# replicates per station
require(reshape)
Param_melt <- melt(Param.mat.df, c("Reps","Station","Replicate"))
Param.mat.df <- cast(Param_melt, formula = Station~variable, mean)

## Making a vector containg selected stations (i.e. stations of interest)
Station <- c(1:24,101:104)
Station <- as.data.frame(Station)

## Merging Param.mat.df and Station
# This selects all the Stations in x, removes the stations in y
# that is not present in x, and gives NA values to stations in x
# not found in y
Param.mat.df <- merge(x = Station, y = Param.mat.df, by.x = "Station",
	by.y = "Station", all.x = TRUE)

## Writing the df to a .txt file
indir = "/home/ost4sem/project/input/"
write.table(Param.mat.df, file = paste(indir, "Grain.txt", sep = ""), sep = " ",
	row.names = FALSE)

############################################################################
############# IMPORTING OTHER ENVIRONMENTAL VARIABLES IN R #################
############################################################################

# Importing to set -9999 as NAs
Vars <- read.table(file = paste(indir, "Variables.txt", sep = ""), header = TRUE, sep = " ",
	na.strings = -9999)

# Writing back to a txt file. Using white space as fieldseparator (thats what bash likes...)
write.table(Vars, file = paste(indir, "VarsNA.txt", sep = ""), sep = " ",
	row.names = FALSE)

# Extracting all the columns containing the smallest sediment fraction (phi=10)
# Also extract the station and replicate
less63.df <- subset(Upper, Upper$phi == 10, select = c("station", "rep", "frequency"))

## Reshaping the data
require(reshape)
Less_melt <- melt(less63.df, c("station","rep"))
less63.df <- cast(Less_melt, formula = station~variable, mean)

## Merge less63.df and Station
less63.df <- merge(x = Station, y = less63.df, by.x = "Station",
	by.y = "station", all.x = TRUE)

# write as txt
write.table(less63.df, file = paste(indir, "less63.txt", sep=""), sep = " ",
	row.names = FALSE)

## Save the image (i.e. all objects)
save.image(file = paste(indir, "Predictors.rdata", sep = ""))

########################################################################
########### COMBINING TEXT FILES INTO ENVMAT.CSV USING BASH ############
########################################################################

# Make sure the files have a space as field separator
head -1 Grain.txt
head -1 VarsNA.txt
head -1 less63.txt

# Change the fieldseparator of Grainsize.csv
# the first gsub argument is the old fs and the second the new fs
### NB! Bash always work with space separated files
# awk '{ gsub("," , " ") ; print }' Grainsize.csv > GrainSize.csv
# rm Grainsize.csv 

## sorting the common field of both files
# Using the sort function alone moves the header information
# and puts it somewhere else
# sort -k 1,1 GrainSize.csv > Grainsize_s.csv
# sort -k 3,3 EnvVar.csv > EnvVar_s.csv

## The following using awk and sort keeps the header untouched
awk 'NR==1; {if(NR > 1) {print $0 | "sort -k 1,1"}}' Grain.txt > GrainS.txt
awk 'NR==1; {if(NR > 1) {print $0 | "sort -k 3,3"}}' VarsNA.txt > VarsNAS.txt
awk 'NR==1; {if(NR > 1) {print $0 | "sort -k 1,1"}}' less63.txt > less63S.txt

## Check if the sort worked correctly
cat  GrainS.txt
cat  VarsNAS.txt
cat  less63S.txt

## Join first VarsNAS and less63S
join --header -1 3 -2 1 VarsNAS.txt less63S.txt > Temp.txt
## Then join Temp.txt and GrainS
join --header -1 1 -2 1 Temp.txt GrainS.txt > EnvMatUS.txt

## Sorting EnvMatUS to get the stations in numerical order
awk 'NR==1; {if(NR > 1) {print $0 | "sort -nk 1,1"}}' EnvMatUS.txt > EnvMat.txt


########################################################################
############## POPULATING MY PREDICTOR DATASET IN GRASS ################
########################################################################

######## Setting the GRASS variables in Bash is not working... #########

## Setting GRASS variables in BASH
#echo "LOCATION_NAME: Stuteberget" 	> $HOME/.grassrc6
#echo "MAPSET: thijs"            	>> $HOME/.grassrc6
#echo "DIGITIZER: none"        		>> $HOME/.grassrc6
#echo "GRASS_GUI: -text"        			>> $HOME/.grassrc6
#echo "GISDBASE: /home/thijs/a/grassdata"      >> $HOME/.grassrc6

# path to GRASS binaries and libraries:  
# export GISBASE=/usr/lib/grass64
#export GISBASE=/opt/grass-42555
#export PATH=$PATH:$GISBASE/bin:$GISBASE/scripts:/opt/grass/bin
#export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:"$GISBASE/lib"
#export GISRC=/home/thijs/.grassrc6
#export GRASS_LD_LIBRARY_PATH=/opt/grass-42555/lib
# use process ID (PID) as lock file number: 
#export GIS_LOCK=$$


## Opening grass
grass64 -wxpython /home/thijs/a/grassdata/Stuteberget/thijs

## Checking the region
g.region -p

## Checking available rasters
g.list rast


########################################################################
####### POPULATING THE EnvMat.txt FILE  WITH PREDICTOR VARIABLES #######
###################### BASH SCRIPTING OF GRASS #########################
########################################################################
### 			scriptname: vwhatrast.sh
########################################################################
###			1. Import Stations to GRASS		     ###
###			2. v.what.rast in a loop		     ###
########################################################################

#!/bin/sh

DIR=/home/thijs/a/ost4sem
INDIR=/home/thijs/a/ost4sem/input
OUTDIR=/home/thijs/a/ost4sem/output

## After saving the filename, do:
#chmod u+x /home/thijs/a/grassdata/Stuteberget/thijs/vwhatrast.sh

## In the terminal, do:
# export GRASS_BATCH_JOB=/home/thijs/a/grassdata/Stuteberget/thijs/vwhatrast.sh ; grass64 -text /home/thijs/a/grassdata/Stuteberget/thijs
## To deactivate the batch job mode, run:
# unset GRASS_BATCH_JOB

cd $INDIR

## Checking available vector files
# g.list vect

## Extracting and appending two vectors containg station information
v.out.ascii SamplingStations@thijs fs=, > Stations.csv
v.out.ascii BigStations_new@PERMANENT fs=, > BigStations.csv
echo "Reading Stations.csv to the terminal"
echo "------------------------------------"
cat Stations.csv
echo "------------------------------------"
echo "Reading BigStations.csv to the terminal"
echo "------------------------------------"-5
cat BigStations.csv
echo "------------------------------------"
# Changing the numbering of the stations in BigStations.csv
awk -F "," '{ OFS="," ; print $1, $2, $3+100 }' BigStations.csv >> Stations.csv
echo "Reading both files when appended"
echo "--------------------------------"
cat Stations.csv
echo "END ----------------------------"

## Importing Stations.txt
v.in.ascii input=$INDIR/Stations.csv output=EnvMatCop fs=, columns="utm_x double, utm_y double, station int" --o
echo "Printing the attribute table of the enewly created vector"
echo "---------------------------------------------------------"
db.select EnvMatCop
echo "END------------------------------------------------------"

## Populating the EnvMatCop using v.what.rast
# First, add columns to the attribute table
v.db.addcol map=EnvMatCop columns="depth double, ac3 int, ac4 int, surmax double, bpi25 double, maxcur25 double"
echo "Checking if the operation was successful"
echo "----------------------------------------"
db.select EnvMatCop
echo "END-------------------------------------"

# Create vectors or arrays to hold the information over which to loop
echo "depth ac3 ac4 surmax bpi25 maxcur25" > colnames.txt
rastnames="bathy_background 3cl_12.5m 4cl_12.5m speed_sur_max_stut BPI_25 maxic_25"

## Looping v.what.rast over all rasters and colnames in a one-to-one way
col=0
for raster in `echo $rastnames` ; do
	col=$((col+1))
	colnames=`awk -v col=$col '{ print $col }' colnames.txt`
	echo $raster  $colnames  
	v.what.rast vect=EnvMatCop rast=$raster col=$colnames
done
echo "-------------------------"
echo "Checking the output again"
echo "-------------------------"
v.db.select EnvMatCop
echo "END ---------------------"

## Write it out as a csv file
v.out.ascii EnvMatCop fs=, columns=utm_x,utm_y,station,depth,ac3,ac4,surmax,bpi25,maxcur25 > EnvMat2.csv
echo "-----------------------------------"
echo "Checking if the file is good"
echo "-----------------------------------"
cat EnvMat2.csv
echo "END--------------------------------"

## It seems like the file lost its header through v.out.ascii, this can easily be added in R.
########################################################################


########################################################################
############## IMPORTING SPECIES ABUNDANCE MATRIX IN R #################
########################################################################
### MOLLUSCA ###
## Created by T. van Son June 14, 2010

## Load the saved image from above
load(file = "/home/thijs/a/ost4sem/input/Predictors.rdata")

## Importing the Mollusca Matrix
setwd("/home/thijs/a/PhD_data/Fielddata/SpeciesData/Mollusca")
Moll <- read.csv(file = "Mollusca_matrix.csv", header = TRUE)
names(Moll)
# [1] "Sta"   "Gr"      "StaGr" "Abr.alb" "Abr.nit" "Apo.pes" "Arc.isl"
# [8] "Ant.ent" "Ast.ell" "Ast.mon" "Ast.sul" "Ast.sp"  "Buc.und" "Buc.sp" 
# [15] "Cor.gib" "Cus.cus" "Cyl.alb" "Dia.min" "Eus.mon" "Hia.arc" "Kur.bid"
# [22] "Lep.cae" "Mac.cal" "Mya.tru" "Mya.sp"  "Myt.edu" "Nuc.ten" "Nuc.per"
# [29] "Per.min" "Per.ova" "Pse.sep" "Ris_fam" "Tel.fer" "Thy.equ" "Thy.fle"
# [36] "Thy.gou" "Thy.sar" "Thy.spp" "Thy.sp"  "Biv.ind" "Gas.ind"
str(Moll)

## Removing Biv.ind, Gas.ind, Thy.spp, and Thy.sp columns
Moll <- Moll[, 1:37]
names(Moll)
str(Moll)
dim(Moll)	# 139 37

## In the df Moll the data is listed per grab pr station
## Summarising the grabs per station and store it in MollSt

## This works for station one
sapply(Moll[Moll$Sta == 1, 4:39], na.rm = TRUE, FUN = sum)
as.vector(sapply(Moll[Moll$Sta == 1, 4:39], na.rm = TRUE, FUN = sum))

## Unique stations
AllStations <- unique(Moll$Sta)
AllSpecies <- names(Moll[, 4:37])
p <- length(AllStations)
MollSt <- matrix(data = NA, nrow = length(AllStations), ncol = length(AllSpecies))

## Looping over all stations
# the sapply function needs to be wrapped by as.vector, otherwise it will return
# the names of the species along with the sum of the species per station. Compare
# the sapply code with and without the as.vector wrapping (See above).

  for (i in seq(length = p)) {
    Sta.i <- AllStations[i]
    temp <- as.vector(sapply(Moll[Moll$Sta == Sta.i, 4:37], na.rm = TRUE, FUN = sum))
    MollSt[i, ] <- temp		# for each iteration row i is fed into MollSt
    }

## Coercing into a dataframe
MollSt <- as.data.frame(MollSt, row.names = AllStations)
colnames(MollSt) <- AllSpecies
str(MollSt)
names(MollSt)

## Adding the Stations to the dataframe MollSt
MollSt$Station <- c(1:24, 101:104)


## Calculating species abundance and richness per station
AbundSt <- as.vector(rowSums(MollSt, na.rm = TRUE))
AbundSt
# Coercing into a dataframe and adding stations
AbundSt <- as.data.frame(AbundSt)
AbundSt$Station <- Station

## Calculating species richness
DiversSt <- as.vector(rowSums(MollSt > 0, na.rm = TRUE))
DiversSt
# Coercing into a dataframe and adding stations
DiversSt <- as.data.frame(DiversSt)
DiversSt$station <- c(1:24, 101:104)
";i:1;s:4:"bash";i:2;N;}i:2;i:3436;}i:284;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3436;}i:285;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:18130;}i:286;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:18132;}i:287;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:18133;}i:288;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:18133;}i:289;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:15:"DATA PROCESSING";i:1;i:2;i:2;i:18133;}i:2;i:18133;}i:290;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:18133;}i:291;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:18133;}i:292;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:18161;}i:293;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:18163;}i:294;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:18169;}i:295;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:13205:"

########################################################################
########################################################################
####################### MODEL FITTING IN R #############################
########################################################################
########################################################################

################## FIRST ATTEMPT WITH POINT DATA #######################
EnvMat.df <- read.table(file = paste(indir, "EnvMat.txt", sep = ""), header = TRUE)

## Checking for collinearity
# Reading in some pair functions
panel.hist <- function(x, ...)
     {
         usr <- par("usr"); on.exit(par(usr))
         par(usr = c(usr[1:2], 0, 1.5) )
         h <- hist(x, plot = FALSE)
         breaks <- h$breaks; nB <- length(breaks)
         y <- h$counts; y <- y/max(y)
         rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
     }
panel.cor <- function(x, y, digits=2, prefix="", cex.cor, ...)
     {
         usr <- par("usr"); on.exit(par(usr))
         par(usr = c(0, 1, 0, 1))
         r <- abs(cor(x, y))
         txt <- format(c(r, 0.123456789), digits=digits)[1]
         txt <- paste(prefix, txt, sep="")
         if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
         text(0.5, 0.5, txt, cex = cex.cor * r)
     }

# Plotting a nice pair plot to check for collinearity
pairs(EnvMat.df[, 4:11], na.action = na.omit, diag.panel = panel.hist, upper.panel = panel.smooth,
	lower.panel = panel.cor)
# na.action is not working...

str(EnvMat.df)
str(DiversSt)

## Merging EnvMat.df and DiverSt
Richness.df <- merge(DiversSt, EnvMat.df)

## Changing the variable name of the richness column
names(Richness.df)[2] <- "rich"

########################################################################
########## FITTING A GLM MODEL USING POINT DATA PREDICTORS #############
########################################################################

## First fit
fit <- glm(rich ~ toc + pendep + bmd2g + frequency + Sorting,
	data = Richness.df)
summary(fit)
plot(fit)
# Shapiro test - test for normality
shapiro.test(resid(fit))
# Histogram of richness
hist(Richness.df$rich, breaks=15)
# updating the model
# .~. -the first point is to explain the same response, ~ means as a 
# function of the same respones var, the last point tells to use the
# same expl vars as before -frequency
fit2 <- update(fit, .~. -frequency)
summary(fit2)
anova(fit, fit2, test = "F")

# Looking the model matrix
model.matrix(fit)
coef(fit)
model.matrix(fit) %*% coef(fit) # is the same as the fitted values
fitted(fit)

## Save the image (i.e. all objects)
save.image(file = paste(indir, "Richness.rdata", sep = ""))

########################################################################
########## FITTING A GLM MODEL USING RASTER DATA PREDICTORS #############
########################################################################

load(file = "/home//ost4sem/project/input/Richness.rdata")
EnvMat2.df <- read.csv(file = paste(indir, "EnvMat2.csv", sep = ""), header = F)

## Removing duplicated x,y, and station columns
EnvMat2.df <- EnvMat2.df[, 4:12]

## Adding the column names of EnvMat2.df
colnames(EnvMat2.df) <- c("utm_x", "utm_y", "station", "depth", "ac3", "ac4", "surmax", "bpi25", "maxcur25")
str(EnvMat2.df)

## ac3 and ac4 are actually factors
EnvMat2.df$fac3 <- as.factor(EnvMat2.df$ac3)
EnvMat2.df$fac4 <- as.factor(EnvMat2.df$ac4)

## Checking for collinearity among Predictors
pairs(EnvMat2.df[, c(4, 7:11)], na.action = na.omit, diag.panel = panel.hist, upper.panel = panel.smooth,
	lower.panel = panel.cor)
# maxic25 and bpi25 are highly correlated. Leaving out maxic25 in further analysis

##
str(EnvMat2.df)
str(DiversSt)

# Changing the variable name of the richness
names(DiversSt)[1] <- "rich"

## Merging EnvMat.df and DiverSt
Richness2.df <- merge(EnvMat2.df, DiversSt)

## First fit with the new predictors
fitn <- glm(rich ~ depth + fac3 + fac4 + surmax + bpi25,
	data = Richness2.df)
summary(fitn)
plot(fitn)
# Shapiro test - test for normality
shapiro.test(resid(fitn))
# Histogram of richness
hist(Richness2.df$rich, breaks=15)

# Looking at the model matrix
model.matrix(fitn)
coef(fitn)
model.matrix(fitn) %*% coef(fitn) # is the same as the fitted values
fitted(fitn)

## Save the image (i.e. all objects)
save.image(file = paste(indir, "Richness.rdata", sep = ""))

########################################################################
############### PREDICTING TO NEW DATA/UNSAMPLED AREAS #################
######### BASED ON GLM MODEL AND RASTER POINT PREDICTOR DATA ###########
########################################################################

## Starting GRASS 
grass64 -wxpython /home/user/ost4sem/grassdb/Stuteberget/thijs

## Generating a new mapset to hold new map subsets
g.mapset -c mapset=ost4sem	# the -c option tells GRASS to create a new mapset if the mapset doesnt exist
g.mapset -l
g.gisenv

## Setting/checking the region sttings in GRASS
g.region -p
g.list region
g.region region=QTC_grids -p # 80 x 84 = 6720 cells

## Adding the thijs mapset
g.mapsets addmapset=thijs

## Generating map subsets so that every map has the same extension and res 
r.mapcalc "depth=bathy_background"
r.mapcalc "surmax=speed_sur_max_stut"
r.mapcalc "bpi25=BPI_25"

## Checking the extension and res of the map subsets
r.info surmax

## Start R
R
load(file = "/home/user/ost4sem/project/input/Richness.rdata")
## Load the spgrass6 package
require(spgrass6)
system("g.region region=QTC_grids -p")
## Loading Predictor rasters from GRASS to R
Rasters <- c("depth", "ac3", "ac4", "surmax", "bpi25")
# p <- length(Rasters)
Spdf.names <- c("depth", "ac3", "ac4", "surmax", "bpi25")
Temp.mat <- matrix(data = NA, nrow = 6720, ncol = 5, byrow = T)

# for loop
for (i in 1:5) {
	name <- Spdf.names[i]
	rast <- Rasters[i]
	print(name); print(rast)
	name <<- readRAST6(rast) 	# double arrow assigns the object to the global env
	Temp.mat[, i] <- (name@data[, 1])
}

## Checking the Temp Matrix
head(Temp.mat)

## Coerce into a data frame
Preddata <- as.data.frame(Temp.mat)
colnames(Preddata) <- c("depth", "ac3", "ac4", "surmax", "bpi25")

## ac3 and ac4 are factors
Preddata$fac3 <- as.factor(Preddata$ac3)
Preddata$fac4 <- as.factor(Preddata$ac4)
str(Preddata)
head(Preddata)

## Predicting to new area
prediction <- predict(fitn, newdata = Preddata, type = "response")
str(prediction)

## Adding the predicted data to an existing raster holding
## the geographical information the set region
name$new <- prediction

## Writing raster back to GRASS
writeRAST6(name, "firstpred", zcol="new", overwrite = T)

#########################################################################
################### RANDOMISATION OF PREDICTED DATA #####################
#########################################################################

## "Another" day in the field observing the richness of molluscs
name$rnorm.pred<-rnorm(nrow(name),name$new,sd=sd(resid(fitn)))

## Writing raster back to GRASS 
writeRAST6(name, "rnorm.pred", zcol="rnorm.pred", overwrite=T)

#########################################################################
###### MAKING AN ANIMATION OF THE UNCERTAINTY OF THE PREDICTIONS ########
################## Script modified from Jens Åstrøm #####################
#########################################################################
setwd("/home/user/ost4sem/project/output")

## Setting the region - this also relates to the resolution of the pngs made
system("g.region region=QTC_grids -p")

## Making fifty "trips" to the field collecting each pixel, and write the
## outcome back to GRASS

for(i in 1:50){
name$rnorm<-rnorm(nrow(name),name$new,sd=sd(resid(fitn)))

writeRAST6(name ,paste("rnorm",i,sep="") ,zcol="rnorm" ,overwrite=T)
}

## Making a png pic of every new prediction
system("for i in `seq 1 50`
 do
  r.out.png input=rnorm$i output=rnorm$i
   g.remove rast=rnorm$i
    done")

## Convert all png pics into a movie
system("convert -delay 20 *.png 80r_84c_rnorm.mpg")

#indir = "/home/user/ost4sem/project/input/"
#save.image(file = paste(indir, "Richness.rdata", sep = ""))

########### MAKING A NEW ANIMATION WITH A HIGHER RESOLUTION ###############
## Creating a new mapset to hold rasters within the QTC_grids region with res=1
system("g.mapset -c mapset=stut1m")
system("g.region region=QTC_grids res=1 -p")

## Resample the raster maps to a finer grid using interpolation
## Is not giving the wanted result...
#system("r.resamp.interp input=depth_12.5 output=depth_1")
#system("r.resamp.interp input=surmax_12.5 output=surmax_1")
#system("r.resamp.interp input=bpi25_12.5 output=bpi_1")
#system("r.resamp.interp input=ac3_12.5 output=ac3_1")
#system("r.resamp.interp input=ac4_12.5 output=ac4_1")

## using r.mapcalc to make map subsets
system("r.mapcalc 'ac3_1=ac3_12.5'")
system("r.mapcalc 'ac4_1=ac4_12.5'")
system("r.mapcalc 'depth_1=depth_12.5'")
system("r.mapcalc 'surmax_1=surmax_12.5'")
system("r.mapcalc 'bpi25_1=bpi25_12.5'")

### Predicting to the new map subset raster files
## Loading Predictor rasters from GRASS to R
Rasters <- c("depth_1", "ac3_1", "ac4_1", "surmax_1", "bpi25_1")
# p <- length(Rasters)
Spdf.names <- c("depth", "ac3", "ac4", "surmax", "bpi25")
Temp.mat <- matrix(data = NA, nrow = 1050000, ncol = 5, byrow = T)

# for loop
for (i in 1:5) {
	name <- Spdf.names[i]
	rast <- Rasters[i]
	print(name); print(rast)
	name <<- readRAST6(rast) 	# double arrow assigns the object to the global env
	Temp.mat[, i] <- (name@data[, 1])
}

## Checking the Temp Matrix 
head(Temp.mat)

## Coerce into a data frame
Preddata <- as.data.frame(Temp.mat)
colnames(Preddata) <- c("depth", "ac3", "ac4", "surmax", "bpi25")

## ac3 and ac4 are factors
Preddata$fac3 <- as.factor(Preddata$ac3)
Preddata$fac4 <- as.factor(Preddata$ac4)
str(Preddata)
head(Preddata)

## Predicting to new area
prediction <- predict(fitn, newdata = Preddata, type = "response")
str(prediction)

## Adding the predicted data to an existing raster
name$new <- prediction

## Writing raster back to GRASS
writeRAST6(name, "firstpred_1", zcol="new", overwrite = T)
# Starting Qgis
system("qgis &")

## "Another" day in the field observing the richness of molluscs
name$rnorm.pred<-rnorm(nrow(name),name$new,sd=sd(resid(fitn)))

## Writing raster back to GRASS 
writeRAST6(name, "rnorm.pred_1", zcol="rnorm.pred", overwrite=T)

##############################################################################
############ RANDOMISATION OF PREDICTED DATA OF HIGH RESOLUTION ##############
##############################################################################

### Now, making another 20 "trips" to the field and write it back to GRASS ###
## Now the res=1, so nrows=1000 and ncols=1050
## Only making 20 "trips"

for(i in 1:20){
name$rnorm<-rnorm(nrow(name),name$new,sd=sd(resid(fitn)))

writeRAST6(name ,paste("rnorm",i,sep="") ,zcol="rnorm" ,overwrite=T)
}

## Making a png pic of every new prediction
system("for i in `seq 1 20`
 do
  r.out.png input=rnorm$i output=rnorm$i
   g.remove rast=rnorm$i
    done")

## Convert all png pics into an animation using ImageMagick
system("convert -delay 20 *.png 1000r_1050c_rnorm.mpg")

save.image(file = paste(indir, "Richness.rdata", sep = ""))


########################################################################
############### PREDICTING TO NEW DATA/UNSAMPLED AREAS #################
######### BASED ON GAM MODEL AND RASTER POINT PREDICTOR DATA ###########
########################################################################

load(file = "/home/user/ost4sem/project/input/Richness.rdata")

## Using the Richness2.df
str(Richness2.df)
head(Richness2.df)

## Fitting a gam
require(mgcv)
fitgam <- gam(rich ~ depth + fac3 + fac4 + surmax + bpi25,
	data = Richness2.df)
summary(fitgam)
str(fitgam)
# Shapiro test - test for normality
shapiro.test(resid(fitgam))

# Looking at the model matrix
model.matrix(fitgam)
coef(fitgam)
model.matrix(fitgam) %*% coef(fitgam) # is the same as the fitted values
fitted(fitgam)

## Predicting to new area
prediction.gam <- predict(fitgam, newdata = Preddata, type = "response")
str(prediction.gam)

## Adding the predicted data to an existing raster
name$newgam <- prediction.gam

## Writing raster back to GRASS
writeRAST6(name, "predgam_1", zcol="newgam", overwrite = T)


## Another day in the field observing the richness of molluscs
name$rnorm.predgam<-rnorm(nrow(name),name$newgam,sd=sd(resid(fitgam)))

## Writing raster back to GRASS
writeRAST6(name, "rnormgam_1", zcol="rnorm.predgam", overwrite = T)


### Now, making another fifty "trips" to the field and write it back to GRASS ###
## Now the res=1, so nrows=1000 and ncols=1050
## Only making 20 "trips"
setwd("/home/user/ost4sem/project/output")

for(i in 1:15){
name$rnormgam<-rnorm(nrow(name),name$newgam,sd=sd(resid(fitgam)))

writeRAST6(name ,paste("rnormgam",i,sep="") ,zcol="rnormgam" ,overwrite=T)
}

## Making a png pic of every new prediction
system("for i in `seq 1 15`
 do
  r.out.png input=rnormgam$i output=rnormgam$i
   g.remove rast=rnormgam$i
    done")

## Convert all png pics into an animation using ImageMagick
system("convert -delay 20 *.png 1000r_1050c_rnorm_gam.mpg")
cd ~/ost4sem/project
";i:1;s:1:"r";i:2;N;}i:2;i:18169;}i:296;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:18169;}i:297;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:31385;}i:298;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:31387;}i:299;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:31388;}i:300;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:31388;}i:301;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:18:"CONDENSED WORKFLOW";i:1;i:3;i:2;i:31388;}i:2;i:31388;}i:302;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:31388;}i:303;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:31388;}i:304;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:77:"This is just a selection of the code, the complete output can be found above.";}i:2;i:31418;}i:305;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:31495;}i:306;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:31497;}i:307;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:34:"Preparation of predictor data sets";i:1;i:3;i:2;i:31497;}i:2;i:31497;}i:308;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:31497;}i:309;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:31497;}i:310;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:144:"I had to prepare two predictor data sets. One that was based on point data (EnvMat.txt) and another that was based on raster data (EnvMat2.csv).";}i:2;i:31543;}i:311;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:31687;}i:312;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:5044:"
########################################################################
##### COMBINING TEXT FILES OF POINT DATA INTO EnvMat.txt USING BASH ####
########################################################################

# Make sure the files have a space as field separator
head -1 Grain.txt
head -1 VarsNA.txt
head -1 less63.txt

# Change the fieldseparator of Grainsize.csv
# the first gsub argument is the old fs and the second the new fs
### NB! Bash always work with space separated files
# awk '{ gsub("," , " ") ; print }' Grainsize.csv > GrainSize.csv
# rm Grainsize.csv 

## sorting the common field of both files
# Using the sort function alone moves the header information
# and puts it somewhere else
# sort -k 1,1 GrainSize.csv > Grainsize_s.csv
# sort -k 3,3 EnvVar.csv > EnvVar_s.csv

## The following using awk and sort keeps the header untouched
awk 'NR==1; {if(NR > 1) {print $0 | "sort -k 1,1"}}' Grain.txt > GrainS.txt
awk 'NR==1; {if(NR > 1) {print $0 | "sort -k 3,3"}}' VarsNA.txt > VarsNAS.txt
awk 'NR==1; {if(NR > 1) {print $0 | "sort -k 1,1"}}' less63.txt > less63S.txt

## Check if the sort worked correctly
cat  GrainS.txt
cat  VarsNAS.txt
cat  less63S.txt

## Join first VarsNAS and less63S
join --header -1 3 -2 1 VarsNAS.txt less63S.txt > Temp.txt
## Then join Temp.txt and GrainS
join --header -1 1 -2 1 Temp.txt GrainS.txt > EnvMatUS.txt

## Sorting EnvMatUS to get the stations in numerical order
awk 'NR==1; {if(NR > 1) {print $0 | "sort -nk 1,1"}}' EnvMatUS.txt > EnvMat.txt

###########################################################################
# PREPARING AND POPULATING THE EnvMat2.csv FILE  WITH PREDICTOR VARIABLES #
###################### BASH SCRIPTING OF GRASS ############################
###########################################################################
### 			scriptname: vwhatrast.sh
###########################################################################
###			1. Import Stations to GRASS		     ######
###			2. v.what.rast in a loop		     ######
###########################################################################

#!/bin/sh

DIR=/home/thijs/a/ost4sem
INDIR=/home/thijs/a/ost4sem/input
OUTDIR=/home/thijs/a/ost4sem/output

## After saving the filename, do:
#chmod u+x /home/thijs/a/grassdata/Stuteberget/thijs/vwhatrast.sh

## In the terminal, do:
# export GRASS_BATCH_JOB=/home/thijs/a/grassdata/Stuteberget/thijs/vwhatrast.sh ; grass64 -text /home/thijs/a/grassdata/Stuteberget/thijs
## To deactivate the batch job mode, run:
# unset GRASS_BATCH_JOB

cd $INDIR

## Checking available vector files
# g.list vect

## Extracting and appending two vectors containg station information
v.out.ascii SamplingStations@thijs fs=, > Stations.csv
v.out.ascii BigStations_new@PERMANENT fs=, > BigStations.csv
echo "Reading Stations.csv to the terminal"
echo "------------------------------------"
cat Stations.csv
echo "------------------------------------"
echo "Reading BigStations.csv to the terminal"
echo "------------------------------------"-5
cat BigStations.csv
echo "------------------------------------"
# Changing the numbering of the stations in BigStations.csv
awk -F "," '{ OFS="," ; print $1, $2, $3+100 }' BigStations.csv >> Stations.csv
echo "Reading both files when appended"
echo "--------------------------------"
cat Stations.csv
echo "END ----------------------------"

## Importing Stations.txt
v.in.ascii input=$INDIR/Stations.csv output=EnvMatCop fs=, columns="utm_x double, utm_y double, station int" --o
echo "Printing the attribute table of the enewly created vector"
echo "---------------------------------------------------------"
db.select EnvMatCop
echo "END------------------------------------------------------"

## Populating the EnvMatCop using v.what.rast
# First, add columns to the attribute table
v.db.addcol map=EnvMatCop columns="depth double, ac3 int, ac4 int, surmax double, bpi25 double, maxcur25 double"
echo "Checking if the operation was successful"
echo "----------------------------------------"
db.select EnvMatCop
echo "END-------------------------------------"

# Create vectors or arrays to hold the information over which to loop
echo "depth ac3 ac4 surmax bpi25 maxcur25" > colnames.txt
rastnames="bathy_background 3cl_12.5m 4cl_12.5m speed_sur_max_stut BPI_25 maxic_25"

## Looping v.what.rast over all rasters and colnames in a one-to-one way
col=0
for raster in `echo $rastnames` ; do
	col=$((col+1))
	colnames=`awk -v col=$col '{ print $col }' colnames.txt`
	echo $raster  $colnames  
	v.what.rast vect=EnvMatCop rast=$raster col=$colnames
done
echo "-------------------------"
echo "Checking the output again"
echo "-------------------------"
v.db.select EnvMatCop
echo "END ---------------------"

## Write it out as a csv file
v.out.ascii EnvMatCop fs=, columns=utm_x,utm_y,station,depth,ac3,ac4,surmax,bpi25,maxcur25 > EnvMat2.csv
echo "-----------------------------------"
echo "Checking if the file is good"
echo "-----------------------------------"
cat EnvMat2.csv
echo "END--------------------------------"
";i:1;s:4:"bash";i:2;N;}i:2;i:31694;}i:313;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:31694;}i:314;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:84:"The mollusk richness file was ready from before (or needed only minor modifications)";}i:2;i:36753;}i:315;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:36837;}i:316;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:36837;}i:317;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:52:"Merging the mollusk richness and predictor data set ";}i:2;i:36839;}i:318;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:36891;}i:319;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:1123:"
#######################################################################
####### MERGING THE SPECIES RICHNESS AND THE EnvMat2.csv FILES ########
#######################################################################
EnvMat2.df <- read.csv(file = paste(indir, "EnvMat2.csv", sep = ""), header = F)

## Removing duplicated x,y, and station columns
EnvMat2.df <- EnvMat2.df[, 4:12]

## Adding the column names of EnvMat2.df
colnames(EnvMat2.df) <- c("utm_x", "utm_y", "station", "depth", "ac3", "ac4", "surmax", "bpi25", "maxcur25")
str(EnvMat2.df)

## ac3 and ac4 are actually factors
EnvMat2.df$fac3 <- as.factor(EnvMat2.df$ac3)
EnvMat2.df$fac4 <- as.factor(EnvMat2.df$ac4)

## Checking for collinearity among Predictors
pairs(EnvMat2.df[, c(4, 7:11)], na.action = na.omit, diag.panel = panel.hist, upper.panel = panel.smooth,
	lower.panel = panel.cor)
# maxic25 and bpi25 are highly correlated. Leaving out maxic25 in further analysis

##
str(EnvMat2.df)
str(DiversSt)

# Changing the variable name of the richness
names(DiversSt)[1] <- "rich"

## Merging EnvMat.df and DiverSt
Richness2.df <- merge(EnvMat2.df, DiversSt)
";i:1;s:1:"r";i:2;N;}i:2;i:36898;}i:320;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:38033;}i:321;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:21:"Model parametrization";i:1;i:3;i:2;i:38033;}i:2;i:38033;}i:322;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:38033;}i:323;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:38033;}i:324;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:79:"Fitting GLM and GAM models in R using mollusk richness as the response variable";}i:2;i:38066;}i:325;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:38145;}i:326;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:594:"
########################################################
####### FITTING A GLM TO RASTER DATA PREDICTORS ########
########################################################
fitn <- glm(rich ~ depth + fac3 + fac4 + surmax + bpi25,
	data = Richness2.df)
summary(fitn)
str(fitn)
plot(fitn)

########################################################
####### FITTING A GAM TO RASTER DATA PREDICTORS ########
########################################################
require(mgcv)
fitgam <- gam(rich ~ depth + fac3 + fac4 + surmax + bpi25,
	data = Richness2.df)
summary(fitgam)
str(fitgam)
plot(fitgam)
";i:1;s:1:"r";i:2;N;}i:2;i:38152;}i:327;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:38758;}i:328;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:16:"Model prediction";i:1;i:3;i:2;i:38758;}i:2;i:38758;}i:329;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:38758;}i:330;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:38758;}i:331;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:115:"Prepare new data to predict to in R, and predict to new data/unvisited areas using both the GLM and the GAM models.";}i:2;i:38786;}i:332;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:38901;}i:333;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:3320:"
########################################################################
######### PREPARING AND PREDICTING TO NEW DATA/UNSAMPLED AREAS #########
############ BASED ON GLM MODEL AND RASTER PREDICTOR DATA ##############
########################################################################

## Creating a new mapset to hold rasters within the QTC_grids region with res=1
## Working from within R
system("g.mapset -c mapset=stut1m")
system("g.region region=QTC_grids res=1 -p")

## Resample the raster maps to a finer grid using interpolation
## Is not giving the wanted result...
#system("r.resamp.interp input=depth_12.5 output=depth_1")
#system("r.resamp.interp input=surmax_12.5 output=surmax_1")
#system("r.resamp.interp input=bpi25_12.5 output=bpi_1")
#system("r.resamp.interp input=ac3_12.5 output=ac3_1")
#system("r.resamp.interp input=ac4_12.5 output=ac4_1")

## using r.mapcalc to make map subsets
system("r.mapcalc 'ac3_1=ac3_12.5'")
system("r.mapcalc 'ac4_1=ac4_12.5'")
system("r.mapcalc 'depth_1=depth_12.5'")
system("r.mapcalc 'surmax_1=surmax_12.5'")
system("r.mapcalc 'bpi25_1=bpi25_12.5'")

### Predicting to the new map subset raster files
## Loading Predictor rasters from GRASS to R
Rasters <- c("depth_1", "ac3_1", "ac4_1", "surmax_1", "bpi25_1")
# p <- length(Rasters)
Spdf.names <- c("depth", "ac3", "ac4", "surmax", "bpi25")
Temp.mat <- matrix(data = NA, nrow = 1050000, ncol = 5, byrow = T)

# for loop
for (i in 1:5) {
	name <- Spdf.names[i]
	rast <- Rasters[i]
	print(name); print(rast)
	name <<- readRAST6(rast) 	# double arrow assigns the object to the global env
	Temp.mat[, i] <- (name@data[, 1])
}

## Checking the Temp Matrix 
head(Temp.mat)

## Coerce into a data frame
Preddata <- as.data.frame(Temp.mat)
colnames(Preddata) <- c("depth", "ac3", "ac4", "surmax", "bpi25")

## ac3 and ac4 are factors
Preddata$fac3 <- as.factor(Preddata$ac3)
Preddata$fac4 <- as.factor(Preddata$ac4)
str(Preddata)
head(Preddata)

########################################################
####### PREDICTING TO NEW DATA/UNSAMPLED AREAS #########
########### USING GAM AND RASTER PREDICTORS ############
########################################################
prediction.gam <- predict(fitgam, newdata = Preddata, type = "response")
str(prediction.gam)

## Adding the predicted data to an existing raster
name$newgam <- prediction.gam

## Writing raster back to GRASS
writeRAST6(name, "predgam_1", zcol="newgam", overwrite = T)

########################################################
############## RANDOMIZED "FIELD TRIPS" ################
######### MODIFIED FROM JENS ÅSTRØM'S SCRIPT ###########
########################################################
### making 15 "trips" to the field and write it back to GRASS
## Now the res=1, so nrows=1000 and ncols=1050
## Only making 15 "trips"
setwd("/home/user/ost4sem/project/output")

for(i in 1:15){
name$rnormgam<-rnorm(nrow(name),name$newgam,sd=sd(resid(fitgam)))

writeRAST6(name ,paste("rnormgam",i,sep="") ,zcol="rnormgam" ,overwrite=T)
}

## Making a png pic of every new prediction
system("for i in `seq 1 15`
 do
  r.out.png input=rnormgam$i output=rnormgam$i
   g.remove rast=rnormgam$i
    done")

## Convert all png pics into an animation using ImageMagick
system("convert -delay 20 *.png 1000r_1050c_rnorm_gam.mpg")
cd ~/ost4sem/project

";i:1;s:1:"r";i:2;N;}i:2;i:38908;}i:334;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:42239;}i:335;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:10:"Validation";i:1;i:3;i:2;i:42239;}i:2;i:42239;}i:336;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:42239;}i:337;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:42261;}i:338;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:22:"RESULTS and DISCUSSION";i:1;i:2;i:2;i:42261;}i:2;i:42261;}i:339;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:42261;}i:340;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:42295;}i:341;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:42295;}i:342;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:42295;}i:343;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:29:" GLM and GAM model prediction";}i:2;i:42299;}i:344;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:42328;}i:345;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:42328;}i:346;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:42328;}i:347;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:42328;}i:348;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:29:":wikidk:glm_prediction_1m.png";i:1;s:17:"GLM_prediction_1m";i:2;N;i:3;s:3:"250";i:4;s:3:"250";i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:42330;}i:349;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:42389;}i:350;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:29:":wikidk:gam_prediction_1m.png";i:1;s:17:"GAM_prediction_1m";i:2;N;i:3;s:3:"250";i:4;s:3:"250";i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:42390;}i:351;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:42449;}i:352;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:42449;}i:353;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:42449;}i:354;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:42449;}i:355;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:1;i:3;s:6:"

  * ";}i:2;i:42449;}i:356;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:21:"Randomized GLM output";}i:2;i:42455;}i:357;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:3;i:3;s:21:"Randomized GLM output";}i:2;i:42455;}i:358;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:42476;}i:359;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:42476;}i:360;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:42476;}i:361;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:4;i:3;s:1:"
";}i:2;i:42476;}i:362;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:42476;}i:363;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:23:":wikidk:rnorm_glm23.png";i:1;s:11:"rnorm_glm23";i:2;N;i:3;s:3:"250";i:4;s:3:"250";i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:42478;}i:364;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:42525;}i:365;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:23:":wikidk:rnorm_glm24.png";i:1;s:11:"rnorm_glm24";i:2;N;i:3;s:3:"250";i:4;s:3:"250";i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:42527;}i:366;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:42574;}i:367;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:42574;}i:368;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:42574;}i:369;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:42574;}i:370;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:1;i:3;s:6:"

  - ";}i:2;i:42574;}i:371;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:21:"Randomized GAM output";}i:2;i:42580;}i:372;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:3;i:3;s:21:"Randomized GAM output";}i:2;i:42580;}i:373;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:42601;}i:374;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:42601;}i:375;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:42601;}i:376;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"markdowku_ulists";i:1;b:1;i:2;i:4;i:3;s:1:"
";}i:2;i:42601;}i:377;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:42601;}i:378;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:23:":wikidk:rnorm_gam14.png";i:1;s:11:"rnorm_gam14";i:2;N;i:3;s:3:"250";i:4;s:3:"250";i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:42603;}i:379;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:42650;}i:380;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:23:":wikidk:rnorm_gam15.png";i:1;s:11:"rnorm_gam15";i:2;N;i:3;s:3:"250";i:4;s:3:"250";i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:42651;}i:381;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:42698;}i:382;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:42698;}i:383;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:55:"Insert a map and related table or graphics if available";}i:2;i:42700;}i:384;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:42755;}i:385;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:64:"
Discuss the biological or geographical significance of results
";}i:2;i:42757;}i:386;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:42821;}i:387;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:42823;}i:388;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:42823;}i:389;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:42823;}i:390;a:3:{i:0;s:12:"document_end";i:1;a:0:{}i:2;i:42823;}}